{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Ndyif1Ho9CHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = {\n",
        "    'betweenness_centrality': 'path/to/betweenness_centrality.csv',\n",
        "    'cash_transactions': 'path/to/cash_trxns.csv',\n",
        "    'degree_centrality': 'path/to/degree_centrality.csv',\n",
        "    'eigenvector_centrality': 'path/to/eigenvector_centrality.csv',\n",
        "    'emt_transactions': 'path/to/emt_trxns.csv',\n",
        "    'kyc_data': 'path/to/kyc.csv',\n",
        "    'pagerank_centrality': 'path/to/pagerank.csv',\n",
        "    'wire_transactions': 'path/to/wire_trxns.csv'\n",
        "}\n",
        "\n",
        "# Load datasets\n",
        "datasets = {name: pd.read_csv(path) for name, path in file_paths.items()}\n",
        "\n",
        "# Clean and preprocess data\n",
        "for key, df in datasets.items():\n",
        "    if 'Unnamed: 0' in df.columns:\n",
        "        df.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "    df.columns = df.columns.str.replace(' ', '_').str.lower()\n",
        "    datasets[key] = df"
      ],
      "metadata": {
        "id": "B3mLePf_9F3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO_cSQkB9BHF"
      },
      "outputs": [],
      "source": [
        "def aggregate_transactions_with_country_count(df, transaction_type):\n",
        "    if transaction_type == 'wire':\n",
        "        sender_agg = df.groupby('id_sender').agg(\n",
        "            total_wire_value=('wire_value', 'sum'),\n",
        "            wire_sent_count=('wire_value', 'count')\n",
        "        ).reset_index()\n",
        "        sender_agg['unique_countries_count'] = df.groupby('id_sender')['country_receiver'].nunique().reset_index(drop=True)\n",
        "\n",
        "        receiver_agg = df.groupby('id_receiver').agg(\n",
        "            total_wire_value=('wire_value', 'sum'),\n",
        "            wire_received_count=('wire_value', 'count')\n",
        "        ).reset_index()\n",
        "        receiver_agg['unique_countries_count'] = df.groupby('id_receiver')['country_sender'].nunique().reset_index(drop=True)\n",
        "\n",
        "        return sender_agg, receiver_agg\n",
        "    else:\n",
        "        # For cash and EMT transactions, set unique countries count to 1\n",
        "        if transaction_type == 'cash':\n",
        "            agg_df = df.groupby('cust_id').agg(\n",
        "                total_value=('value', 'sum'),\n",
        "                transaction_count=('value', 'count')\n",
        "            ).reset_index()\n",
        "        else:  # EMT transactions\n",
        "            agg_df = df.groupby('id_sender').agg(\n",
        "                total_emt_value=('emt_value', 'sum'),\n",
        "                emt_sent_count=('emt_value', 'count')\n",
        "            ).reset_index()\n",
        "\n",
        "            receiver_agg = df.groupby('id_receiver').agg(\n",
        "                total_emt_value=('emt_value', 'sum'),\n",
        "                emt_received_count=('emt_value', 'count')\n",
        "            ).reset_index()\n",
        "\n",
        "            agg_df['unique_countries_count'] = 1\n",
        "            receiver_agg['unique_countries_count'] = 1\n",
        "\n",
        "            return agg_df, receiver_agg\n",
        "\n",
        "        agg_df['unique_countries_count'] = 1\n",
        "        return agg_df, None\n",
        "\n",
        "# Aggregate transactions with country count\n",
        "cash_agg, _ = aggregate_transactions_with_country_count(datasets['cash_transactions'], 'cash')\n",
        "emt_sender_agg, emt_receiver_agg = aggregate_transactions_with_country_count(datasets['emt_transactions'], 'emt')\n",
        "wire_sender_agg, wire_receiver_agg = aggregate_transactions_with_country_count(datasets['wire_transactions'], 'wire')\n",
        "\n",
        "def prepare_for_merge(aggregated_df, transaction_type_prefix, role):\n",
        "    exclude_from_renaming = ['cust_id', 'id_sender', 'id_receiver']\n",
        "    renamed_columns = {\n",
        "        col: f\"{transaction_type_prefix}_{role}_{col}\" if col not in exclude_from_renaming else col\n",
        "        for col in aggregated_df.columns\n",
        "    }\n",
        "    return aggregated_df.rename(columns=renamed_columns)\n",
        "\n",
        "prepared_cash_agg = prepare_for_merge(cash_agg, 'cash', 'sender')  # Adjust based on your data structure\n",
        "\n",
        "prepared_emt_sender_agg = prepare_for_merge(emt_sender_agg, 'emt', 'sender')\n",
        "prepared_emt_receiver_agg = prepare_for_merge(emt_receiver_agg, 'emt', 'receiver')\n",
        "\n",
        "prepared_wire_sender_agg = prepare_for_merge(wire_sender_agg, 'wire', 'sender')\n",
        "prepared_wire_receiver_agg = prepare_for_merge(wire_receiver_agg, 'wire', 'receiver')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kyc_cent_df = datasets['kyc_data']\n",
        "for key, cent_df in centrality_dicts.items():\n",
        "    kyc_cent_df = kyc_cent_df.merge(cent_df, how='left', left_on='cust_id', right_on='vertex', suffixes=('', f'_cent_{key}'))\n",
        "    kyc_cent_df.drop('vertex', axis=1, inplace=True)\n",
        "\n",
        "final_df = kyc_cent_df.copy()\n",
        "\n",
        "# Function to join datasets on 'cust_id', handling both sender and receiver cases\n",
        "def join_on_cust_id(base_df, new_df, id_col):\n",
        "    return base_df.merge(new_df, how='left', left_on='cust_id', right_on=id_col)\n",
        "\n",
        "# Join aggregated transaction data with the base dataset\n",
        "for df, id_col in [(prepared_cash_agg, 'cust_id'),\n",
        "                   (prepared_emt_sender_agg, 'id_sender'),\n",
        "                   (prepared_emt_receiver_agg, 'id_receiver'),\n",
        "                   (prepared_wire_sender_agg, 'id_sender'),\n",
        "                   (prepared_wire_receiver_agg, 'id_receiver')]:\n",
        "    final_df = join_on_cust_id(final_df, df, id_col)\n",
        "\n",
        "# Drop duplicate or unnecessary id columns if any\n",
        "final_df.drop(['id_sender', 'id_receiver'], axis=1, errors='ignore', inplace=True)\n",
        "\n",
        "# Save the final DataFrame\n",
        "final_df.to_csv('path/to/final_joined_transactions_with_kyc_and_centrality.csv', index=False)"
      ],
      "metadata": {
        "id": "C7QpMtzN9pfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = final_df.select_dtypes(include=[np.number]).columns\n",
        "final_df[numeric_cols] = final_df[numeric_cols].fillna(0)"
      ],
      "metadata": {
        "id": "xUOV_5Gq9YlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Example using TF-IDF and K-Means clustering\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(final_df['occupation'])\n",
        "\n",
        "num_clusters = 10\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "final_df['occupation_cluster'] = kmeans.fit_predict(X)\n",
        "\n",
        "print(final_df['occupation_cluster'].value_counts())"
      ],
      "metadata": {
        "id": "pjeAIzaT9oxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the 'gender' column\n",
        "final_df['gender_encoded'] = label_encoder.fit_transform(final_df['gender'])\n",
        "\n",
        "# Verify the transformation\n",
        "print(final_df[['gender', 'gender_encoded']].head())"
      ],
      "metadata": {
        "id": "5C4I2gaI9z1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Select numerical columns for standardization\n",
        "numerical_cols = final_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Fit and transform the numerical data\n",
        "final_df[numerical_cols] = scaler.fit_transform(final_df[numerical_cols])\n",
        "\n",
        "# Check the standardized data\n",
        "print(final_df[numerical_cols].head())"
      ],
      "metadata": {
        "id": "SQ-1GLIf94Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest"
      ],
      "metadata": {
        "id": "3vrMocBl97N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = final_df.select_dtypes(include=['float64', 'int64'])"
      ],
      "metadata": {
        "id": "8K6bMzbg-Cg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Isolation Forest model\n",
        "iso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "iso_forest.fit(X)"
      ],
      "metadata": {
        "id": "GA5IIwPV-FsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomalies = iso_forest.predict(X)\n",
        "final_df['anomaly'] = anomalies\n",
        "print(\"Number of anomalies detected:\", list(anomalies).count(-1))"
      ],
      "metadata": {
        "id": "AqSUna4u-INi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomalies_df = final_df[final_df['anomaly'] == -1]\n",
        "\n",
        "# Explore the anomalies\n",
        "print(anomalies_df.head())"
      ],
      "metadata": {
        "id": "6SL32Kfy-KUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols = final_df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "scaler.fit(final_df[numerical_cols])\n",
        "\n",
        "final_df[numerical_cols] = scaler.inverse_transform(final_df[numerical_cols])"
      ],
      "metadata": {
        "id": "vwEvtUlH-P3l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}